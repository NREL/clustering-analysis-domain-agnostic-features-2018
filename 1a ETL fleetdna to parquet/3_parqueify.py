# File: 3_parqueify.sh
# Project: clustering-analysis-domain-agnostic-features-2018
# Authors: Alexander van Roijen, Caleb Phillips
# Copyright (c) 2021 Alliance for Sustainable Energy LLC

from pyspark import SQLContext
from pyspark.sql import DataFrame
from pyspark.sql.functions import lit, col, when
import os.path
from pyspark.sql.types import *
from os import listdir
from os.path import isfile, join
import sys

#pass in the path of both where the csv file is and where you desire the resulting file to be placed and named

sqlContext = SQLContext(sc)

csvFile = sys.argv[0]
outputFile = sys.argv[1]

print("this script assumes there is no header associated with the csvFile generated by combine.sh, edit this script to fit the schema desired")
f = sc.textFile("csvFile")
data_rdd = f.map(lambda line: [x for x in line.split(',')])
df = data_rdd.toDF(['vdir','dtdir','ts','lat','lon','speed','grade']) # this is assuming the order of the fields, be wary to change this accordingly if you add more fields
TBC = [('lat','lat2'),('lon','lon2'),('speed','speed2'),('grade','grade2')] # what you want to be floats and not strings
for p in TBC:
  df = df.withColumn(p[1],col(p[0]).cast("float"))
  df = df.drop(p[0])
df.write.mode('overwrite').parquet(outputFile,partitionBy=['id'])
print "done"

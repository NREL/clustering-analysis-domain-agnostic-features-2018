{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b_Agnostic feature extraction notebook\n",
    "\n",
    "Project: clustering-analysis-domain-agnostic-features-2018\n",
    "\n",
    "Authors: Jordan Perr-Sauer, Caleb Phillips\n",
    "\n",
    "License: BSD 3-Clause\n",
    "\n",
    "Copyright (c) 2021 Alliance for Sustainable Energy LLC\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook uses a Spark context and the TSFresh python library to produce the application agnostic features CSV file from the FleeDNA dataset. The notbook must be run with a spark context. The easiet way to do this is to boot up Spark using start_spark_jupyter_notebook.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'0.20.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "pandas.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tsfresh\n",
    "tsfresh.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.version\n",
    "pickle.HIGHEST_PROTOCOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jordan Perr-Sauer <jordan.perr-sauer@nrel.gov>\n",
    "\n",
    "from pyspark.sql.functions import collect_list, col, struct, array\n",
    "from pyspark.sql.functions import col, size, max, avg\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import StructType, DoubleType, StructField, IntegerType, ArrayType,  MapType, StringType\n",
    "import pandas\n",
    "import tsfresh\n",
    "from tsfresh.feature_extraction.settings import EfficientFCParameters, MinimalFCParameters, ComprehensiveFCParameters\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import json\n",
    "\n",
    "\n",
    "def post_processing(features):\n",
    "    \"\"\"\n",
    "    Normalize features and fill null values with zero.\n",
    "    :param features: Pandas dataframe with an index column \"vdir\" and however many features columns\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df = features.toPandas()\n",
    "    df.set_index(\"vdir\")\n",
    "\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    columns = df.columns.difference(['vdir'])\n",
    "    df[columns] = min_max_scaler.fit_transform(df[columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def tsfresh_udf(params = \"minimal\", features = [\"speed\", \"grade\", \"ts\"]):\n",
    "    \n",
    "    param_dict = {\"minimal\": MinimalFCParameters, \"full\": ComprehensiveFCParameters, \"efficient\": EfficientFCParameters}\n",
    "\n",
    "    def agnostic_features_distributed_pertrip(raw_data):\n",
    "        data = pandas.DataFrame(raw_data)\n",
    "        data.columns = features\n",
    "        data_features = data.fillna(0)\n",
    "        data_features['id']='foo'\n",
    "        extract_agnostic = tsfresh.extract_features(data_features,\n",
    "                                                    column_id=\"id\",\n",
    "                                                    column_sort=\"ts\",\n",
    "                                                    default_fc_parameters=param_dict[params](),\n",
    "                                                    n_jobs=0)\n",
    "        ef = extract_agnostic.replace([np.inf, -np.inf], np.nan)\n",
    "        d = ef.to_dict(orient='records')[0]\n",
    "        dd = {str(key): float(val) for (key, val) in d.items()}\n",
    "        return dd\n",
    "\n",
    "    return agnostic_features_distributed_pertrip\n",
    "\n",
    "\n",
    "# The following functions return feature vectors and should be called by a driver program\n",
    "\n",
    "\n",
    "def halfday_minimal_trip_means(df, averageTo=\"vehicle\", parameters=\"minimal\", maximum_trip=1000):\n",
    "    \"\"\"\n",
    "    Couldn't this be done so much easier with an RDD?\n",
    "\n",
    "    Agnostic Features EPA Prime Full - Trip Means Method\n",
    "    :param fleet:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    features = [\"speed\", \"grade\", \"ts\"]\n",
    "\n",
    "    afd = df.groupBy(\"vdir\", \"trip\").agg(collect_list(struct(*features)).alias(\"raw_data\"))\n",
    "\n",
    "    afd_repartition = afd.repartition(2500)\n",
    "\n",
    "    afd_limited = afd_repartition.withColumn(\"data_length\", size(col(\"raw_data\"))).filter(col(\"data_length\") <= maximum_trip)\n",
    "\n",
    "    agnostic_features_distributed_udf = udf(tsfresh_udf(parameters), MapType(StringType(), DoubleType()))\n",
    "\n",
    "    afd2 = afd_limited.withColumn(\"features\", agnostic_features_distributed_udf(col(\"raw_data\")))\n",
    "\n",
    "    afd2.cache()\n",
    "\n",
    "    keys = afd2.select(explode(\"features\")).select(\"key\").distinct().collect()\n",
    "\n",
    "    if averageTo == \"vehicle\":\n",
    "        averages = [avg(col(\"features\").getItem(k.key)).alias(k.key) for k in keys]\n",
    "        afd3 = afd2.groupBy(\"vdir\").agg(*averages)\n",
    "    else:\n",
    "        trips = [col(\"features\").getItem(k.key).alias(k.key) for k in keys]\n",
    "        afd3 = afd2.select(\"vdir\", \"trip\", *trips)\n",
    "\n",
    "    return post_processing(afd3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"./data/FleetDNAETL_CoDA_epaprime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnostic_pandas = halfday_minimal_trip_means(df, parameters=\"efficient\", maximum_trip=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agnostic_pandas.to_csv(\"./data/FleetDNAETL_CoDA_epaprime_agnostic_50klimit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python395jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "name": "2b Agnostic Feature Extraction",
  "notebookId": 5007
 },
 "nbformat": 4,
 "nbformat_minor": 1
}